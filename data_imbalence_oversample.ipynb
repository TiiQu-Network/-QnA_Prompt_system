{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fee207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as no\n",
    "# Filter out DeprecationWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# below is loading dataset. put own path if csv file is not in same folder\n",
    "\n",
    "f=\"8606 db for prelabelling  - db.csv\"\n",
    "\n",
    "\n",
    "df1=pd.read_csv(f,skiprows=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc96d85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8479 entries, 0 to 8478\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Question  8479 non-null   object\n",
      " 1   Answer    8477 non-null   object\n",
      " 2   Final     8479 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 198.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df=df1[[\"Question\",\"Answer\",\"Final\"]] # consider question, answer and final label columns only \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d30b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final\n",
       "analysis                 2911\n",
       "science and tech         2025\n",
       "Tie                      1848\n",
       "factual                   726\n",
       "strategy                  725\n",
       "management                 84\n",
       "taxonomy                   80\n",
       "ethics and regulation      52\n",
       "Science and Tech            5\n",
       "incomplete Q&A              5\n",
       "analysis                    3\n",
       "Analysis                    3\n",
       "Taxonomy                    3\n",
       "science and tech            2\n",
       "Management                  2\n",
       "Factual                     2\n",
       "taxonomy                    1\n",
       "factual                     1\n",
       "incomplete                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Final\"].value_counts() # Will remove Tie and add the labels into one final label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843c261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'text1' column contains the specified value\n",
    "df = df[df['Final'] != 'Tie']\n",
    "\n",
    "# Print the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1882bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.dropna() # Drop Nans rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e1ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['joined_column'] = df2['Question'] + ' ' + df2['Answer'] # combine question and answer into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b882379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2[[\"joined_column\",\"Final\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e3d3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final\n",
       "analysis                 2911\n",
       "science and tech         2025\n",
       "strategy                  725\n",
       "factual                   724\n",
       "management                 84\n",
       "taxonomy                   80\n",
       "ethics and regulation      52\n",
       "Science and Tech            5\n",
       "incomplete Q&A              5\n",
       "Taxonomy                    3\n",
       "analysis                    3\n",
       "Analysis                    3\n",
       "science and tech            2\n",
       "Management                  2\n",
       "Factual                     2\n",
       "factual                     1\n",
       "incomplete                  1\n",
       "taxonomy                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"Final\"].value_counts() # final count of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74764a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df3[(df3[\"Final\"] == 'strategy') | (df3[\"Final\"] == 'science and tech') | (df3[\"Final\"] == 'analysis')\n",
    "       | (df3[\"Final\"] == 'factual') | (df3[\"Final\"] == 'taxonomy') | (df3[\"Final\"] == 'management')\n",
    "       | (df3[\"Final\"] == 'ethics and regulation')] # consider only these labels , can correct other spelling mistakes \n",
    "                                                    # into parent one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7bbc81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joined_column</th>\n",
       "      <th>Final</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What were the sources of atmospheric nutrients...</td>\n",
       "      <td>analysis</td>\n",
       "      <td>source atmospheric nutrient tianchi lake prima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How was the fertilization effect on phytoplank...</td>\n",
       "      <td>analysis</td>\n",
       "      <td>fertilization effect phytoplankton expected ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do rising temperatures affect Alpine lakes...</td>\n",
       "      <td>science and tech</td>\n",
       "      <td>rising temperature affect alpine lake rising t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How has an increase in phytoplankton biomass b...</td>\n",
       "      <td>analysis</td>\n",
       "      <td>increase phytoplankton biomass observed alpine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do higher metabolic rates of organisms and...</td>\n",
       "      <td>science and tech</td>\n",
       "      <td>higher metabolic rate organism longer growing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       joined_column             Final   \n",
       "0  What were the sources of atmospheric nutrients...          analysis  \\\n",
       "1  How was the fertilization effect on phytoplank...          analysis   \n",
       "2  How do rising temperatures affect Alpine lakes...  science and tech   \n",
       "3  How has an increase in phytoplankton biomass b...          analysis   \n",
       "4  How do higher metabolic rates of organisms and...  science and tech   \n",
       "\n",
       "                                                text  \n",
       "0  source atmospheric nutrient tianchi lake prima...  \n",
       "1  fertilization effect phytoplankton expected ch...  \n",
       "2  rising temperature affect alpine lake rising t...  \n",
       "3  increase phytoplankton biomass observed alpine...  \n",
       "4  higher metabolic rate organism longer growing ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list text\n",
    "\n",
    "text = list(df4['joined_column'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing loop\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "\n",
    "#Remove all special characters\n",
    "#Lowercase all the words\n",
    "#Tokenize\n",
    "#Remove stopwords\n",
    "#Lemmatize\n",
    "\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('english')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#assign corpus to data['text']\n",
    "\n",
    "df4['text'] = corpus\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b178c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data for class labels\n",
    "y = ['analysis', 'ethics and regulation', 'factual', 'management', 'science and tech', 'strategy', 'taxonomy']\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on your class labels and transform them into integer labels\n",
    "y_encoded = label_encoder.fit_transform(df4[\"Final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6db626",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df4[\"text\"], y_encoded, test_size=0.2, random_state=42)\n",
    "# split train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31152b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to numerical features using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ec307af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply oversampling to balance the training data\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train_tfidf, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52685639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.74110522331567\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.81      0.79       608\n",
      "           1       0.50      0.29      0.36         7\n",
      "           2       0.80      0.70      0.75       152\n",
      "           3       0.50      0.19      0.27        16\n",
      "           4       0.71      0.74      0.72       381\n",
      "           5       0.62      0.62      0.62       141\n",
      "           6       0.83      0.31      0.45        16\n",
      "\n",
      "    accuracy                           0.74      1321\n",
      "   macro avg       0.68      0.52      0.57      1321\n",
      "weighted avg       0.74      0.74      0.74      1321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report,make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# setting class_weight balenced helps with data imbalenced issues \n",
    "\n",
    "random_forest = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "logistic_regression1 = LogisticRegression(class_weight='balanced',max_iter=7600)\n",
    "xgboost_classifier1 = XGBClassifier(n_estimators=300)\n",
    "\n",
    "# Create a voting classifier with 'soft' voting\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', logistic_regression1),\n",
    "        ('xg', xgboost_classifier1),\n",
    "        ('dt', random_forest),\n",
    "       \n",
    "      \n",
    "        \n",
    "    ],\n",
    "    voting='soft'  # Use 'soft' voting for probability-based decision\n",
    ")\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred1 = voting_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy1 = accuracy_score(y_test, y_pred1)\n",
    "print(\"Voting Classifier Accuracy:\", accuracy1)\n",
    "\n",
    "# Evaluate the models\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a34da6",
   "metadata": {},
   "source": [
    "- RandomOverSampler and weight=\"balenced\" helps improve the data imbalenced issue metrics by a bit. This shows these methods should be considered and used to counter the data imbalence issue in the dataset.\n",
    "- Also there are other data imbalenced methods that can be explored aswell\n",
    "\n",
    "- the less imbalenced dataset issues metrics seem better than previous attempt overall when randomoversampler was not used as it takes into account data imbalence when training the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf92991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca45e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

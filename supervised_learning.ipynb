{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3527a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "f=\"8606 db for prelabelling  - db.csv\"\n",
    "\n",
    "\n",
    "df1=pd.read_csv(f,skiprows=11)\n",
    "\n",
    "#Read the dataset, skip first 11 rows as was empty space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609af1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8479 entries, 0 to 8478\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Question  8479 non-null   object\n",
      " 1   Answer    8477 non-null   object\n",
      " 2   Final     8479 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 198.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df=df1[[\"Question\",\"Answer\",\"Final\"]]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec64d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing tie labels as it is unknown, and will affect model performance for now\n",
    "df = df[df['Final'] != 'Tie']\n",
    "\n",
    "# Print the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003dcddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6631 entries, 0 to 8477\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Question  6631 non-null   object\n",
      " 1   Answer    6629 non-null   object\n",
      " 2   Final     6631 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 207.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbc0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.dropna() # Drop Nans rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77dcb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this appraoch, will combine question and answer into one column. \n",
    "#will experiment with both the columns as seperate features and so on in future iterations\n",
    "\n",
    "df2['joined_column'] = df['Question'] + ' ' + df['Answer'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40d02993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2[[\"joined_column\",\"Final\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4b70dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Final\n",
       "analysis                 2911\n",
       "science and tech         2025\n",
       "strategy                  725\n",
       "factual                   724\n",
       "management                 84\n",
       "taxonomy                   80\n",
       "ethics and regulation      52\n",
       "Science and Tech            5\n",
       "incomplete Q&A              5\n",
       "Taxonomy                    3\n",
       "analysis                    3\n",
       "Analysis                    3\n",
       "science and tech            2\n",
       "Management                  2\n",
       "Factual                     2\n",
       "factual                     1\n",
       "incomplete                  1\n",
       "taxonomy                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"Final\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b045f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=df3[(df3[\"Final\"] == 'strategy') | (df3[\"Final\"] == 'science and tech') | (df3[\"Final\"] == 'analysis')\n",
    "       | (df3[\"Final\"] == 'factual') | (df3[\"Final\"] == 'taxonomy') | (df3[\"Final\"] == 'management')\n",
    "       | (df3[\"Final\"] == 'ethics and regulation')] # only consider the main labels, can combine the mispelled into parent \n",
    "                                                    # spelling label. ALSO, data is unbalenced for ethics,taxonomy and managment mainly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe91a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df4[df4[\"Final\"]==\"taxonomy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07e6f623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joined_column</th>\n",
       "      <th>Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>source atmospheric nutrient tianchi lake prima...</td>\n",
       "      <td>analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fertilization effect phytoplankton expected ch...</td>\n",
       "      <td>analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rising temperature affect alpine lake rising t...</td>\n",
       "      <td>science and tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>increase phytoplankton biomass observed alpine...</td>\n",
       "      <td>analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>higher metabolic rate organism longer growing ...</td>\n",
       "      <td>science and tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       joined_column             Final\n",
       "0  source atmospheric nutrient tianchi lake prima...          analysis\n",
       "1  fertilization effect phytoplankton expected ch...          analysis\n",
       "2  rising temperature affect alpine lake rising t...  science and tech\n",
       "3  increase phytoplankton biomass observed alpine...          analysis\n",
       "4  higher metabolic rate organism longer growing ...  science and tech"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() # lemma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "\n",
    "text = list(df4['joined_column']) # for cleaning purposes\n",
    "\n",
    "#below is following:\n",
    "#Remove all special characters\n",
    "#Lowercase all the words\n",
    "#Tokenize\n",
    "#Remove stopwords\n",
    "#Lemmatize\n",
    "\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i]) \n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('english')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#assign corpus to data['text']\n",
    "\n",
    "df4['joined_column'] = corpus\n",
    "\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef66595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Logistic Regression\n",
      "Best Parameters: {'C': 1.0}\n",
      "Accuracy: 0.7320\n",
      "Precision: 0.7256\n",
      "Recall: 0.7320\n",
      "F1-score: 0.7194\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             analysis       0.74      0.86      0.79       608\n",
      "ethics and regulation       0.00      0.00      0.00         7\n",
      "              factual       0.86      0.63      0.73       152\n",
      "           management       1.00      0.19      0.32        16\n",
      "     science and tech       0.68      0.73      0.70       381\n",
      "             strategy       0.71      0.50      0.59       141\n",
      "             taxonomy       0.00      0.00      0.00        16\n",
      "\n",
      "             accuracy                           0.73      1321\n",
      "            macro avg       0.57      0.41      0.45      1321\n",
      "         weighted avg       0.73      0.73      0.72      1321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classifier: K-Nearest Neighbors\n",
      "Best Parameters: {'n_neighbors': 7}\n",
      "Accuracy: 0.6048\n",
      "Precision: 0.6392\n",
      "Recall: 0.6048\n",
      "F1-score: 0.5614\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             analysis       0.57      0.89      0.70       608\n",
      "ethics and regulation       0.00      0.00      0.00         7\n",
      "              factual       0.89      0.52      0.66       152\n",
      "           management       0.67      0.25      0.36        16\n",
      "     science and tech       0.61      0.44      0.51       381\n",
      "             strategy       0.83      0.07      0.13       141\n",
      "             taxonomy       0.00      0.00      0.00        16\n",
      "\n",
      "             accuracy                           0.60      1321\n",
      "            macro avg       0.51      0.31      0.34      1321\n",
      "         weighted avg       0.64      0.60      0.56      1321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classifier: Decision Tree\n",
      "Best Parameters: {'max_depth': 20}\n",
      "Accuracy: 0.6276\n",
      "Precision: 0.6316\n",
      "Recall: 0.6276\n",
      "F1-score: 0.6074\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             analysis       0.61      0.85      0.71       608\n",
      "ethics and regulation       0.00      0.00      0.00         7\n",
      "              factual       0.89      0.49      0.64       152\n",
      "           management       0.00      0.00      0.00        16\n",
      "     science and tech       0.63      0.46      0.54       381\n",
      "             strategy       0.57      0.40      0.47       141\n",
      "             taxonomy       0.40      0.12      0.19        16\n",
      "\n",
      "             accuracy                           0.63      1321\n",
      "            macro avg       0.44      0.33      0.36      1321\n",
      "         weighted avg       0.63      0.63      0.61      1321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classifier: Random Forest\n",
      "Best Parameters: {'n_estimators': 200}\n",
      "Accuracy: 0.7033\n",
      "Precision: 0.7010\n",
      "Recall: 0.7033\n",
      "F1-score: 0.6899\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             analysis       0.73      0.82      0.77       608\n",
      "ethics and regulation       0.00      0.00      0.00         7\n",
      "              factual       0.79      0.65      0.71       152\n",
      "           management       0.50      0.06      0.11        16\n",
      "     science and tech       0.65      0.71      0.68       381\n",
      "             strategy       0.66      0.43      0.52       141\n",
      "             taxonomy       1.00      0.06      0.12        16\n",
      "\n",
      "             accuracy                           0.70      1321\n",
      "            macro avg       0.62      0.39      0.42      1321\n",
      "         weighted avg       0.70      0.70      0.69      1321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Classifier: Support Vector Machine\n",
      "Best Parameters: {'C': 1.0, 'kernel': 'linear'}\n",
      "Accuracy: 0.7207\n",
      "Precision: 0.7234\n",
      "Recall: 0.7207\n",
      "F1-score: 0.7108\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             analysis       0.74      0.84      0.79       608\n",
      "ethics and regulation       0.00      0.00      0.00         7\n",
      "              factual       0.86      0.61      0.71       152\n",
      "           management       0.67      0.12      0.21        16\n",
      "     science and tech       0.67      0.71      0.69       381\n",
      "             strategy       0.67      0.54      0.60       141\n",
      "             taxonomy       1.00      0.12      0.22        16\n",
      "\n",
      "             accuracy                           0.72      1321\n",
      "            macro avg       0.66      0.42      0.46      1321\n",
      "         weighted avg       0.72      0.72      0.71      1321\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,classification_report,make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Preprocessing steps\n",
    "X = df4['joined_column']\n",
    "y = df4['Final']\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define classifiers and their hyperparameter grids\n",
    "classifiers = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=7600), {'C': [0.1, 1.0, 10.0]}),\n",
    "    'K-Nearest Neighbors': (KNeighborsClassifier(), {'n_neighbors': [3, 5, 7]}),\n",
    "    'Decision Tree': (DecisionTreeClassifier(), {'max_depth': [None, 10, 20]}),\n",
    "    'Random Forest': (RandomForestClassifier(), {'n_estimators': [50, 100, 200]}),\n",
    "    'Support Vector Machine': (SVC(), {'C': [0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf']})\n",
    "}\n",
    "\n",
    "# Define evaluation metrics\n",
    "scoring = {\n",
    "    'Accuracy': make_scorer(accuracy_score),\n",
    "    'Precision': make_scorer(precision_score, average='weighted'),\n",
    "    'Recall': make_scorer(recall_score, average='weighted'),\n",
    "    'F1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Perform grid search for each classifier\n",
    "results = {}\n",
    "\n",
    "for classifier_name, (classifier, param_grid) in classifiers.items():\n",
    "    grid_search = GridSearchCV(classifier, param_grid, scoring=scoring, refit='F1', cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Store results\n",
    "    results[classifier_name] = {\n",
    "        'Best Parameters': grid_search.best_params_,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'F1': f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'Classification Report': classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0)\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for classifier_name, metrics in results.items():\n",
    "    print(f'Classifier: {classifier_name}')\n",
    "    print(f'Best Parameters: {metrics[\"Best Parameters\"]}')\n",
    "    print(f'Accuracy: {metrics[\"Accuracy\"]:.4f}')\n",
    "    print(f'Precision: {metrics[\"Precision\"]:.4f}')\n",
    "    print(f'Recall: {metrics[\"Recall\"]:.4f}')\n",
    "    print(f'F1-score: {metrics[\"F1\"]:.4f}')\n",
    "    print(f'Classification Report:\\n{metrics[\"Classification Report\"]}')\n",
    "    print('\\n')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674090bd",
   "metadata": {},
   "source": [
    "\n",
    "#This was the dataset labels size for this approach \n",
    "\n",
    "analysis                 2911\n",
    "\n",
    "science and tech         2025\n",
    "\n",
    "strategy                  725\n",
    "\n",
    "factual                   724\n",
    "\n",
    "management                 84\n",
    "\n",
    "taxonomy                   80\n",
    "\n",
    "ethics and regulation      52\n",
    "\n",
    "Key points:\n",
    "\n",
    "- Tie labels were not considered for this approach but can be intergrating in future appraoches with relavant solutions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Taxonomy, Management and ethics and regulation had low precision and recall scores. This is due to them having less\n",
    "  labeled data. \n",
    "  \n",
    "  \n",
    "- analysis, factual, science and tech are getting decent results but strategy could be better.\n",
    "\n",
    "\n",
    "- Need to integrate data imbalenced approaches to the models\n",
    "\n",
    "\n",
    "- Code will need to be revised aswell in future iterations\n",
    "\n",
    "\n",
    "###  This is an initial baseline draft version, can build and improve this further by trying different pre-processing, cleaning, features, models and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2f8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
